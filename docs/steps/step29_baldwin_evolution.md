# Step 29: Baldwin 进化 — 先天拓扑优化

> 日期: 2026-02-08
> 状态: ✅ 完成
> 核心突破: 泛化 +0.009 → +0.667 (74×提升!)

## 问题诊断: 之前两次进化都失败

- Step 16: eval_steps=2000, 优化了"短期随机表现"
- Step 28: eval_steps=200, lgn_baseline=17 导致 agent 冻住 (短评估陷阱)
- 根因: 适应度选择"绝对表现"而非"学习能力"

## Baldwin 效应适应度函数 (修正)

```
fitness = improvement × 3.0    // 核心: 学得快 >> 天生就会
        + late_safety × 1.0    // 辅助: 最终也要好
        + food × 0.001 - danger × 0.001
```

关键设计:
- improvement 权重 3.0 (之前 2.0) → 强制选择"能学习的大脑"
- 5 个 seed (之前 2-3) → 防止过拟合特定地图
- 1000 步评估 (之前 200 或 5000) → 足够碰到食物但不太慢
- 早期终止: 0 food AND 0 danger → frozen agent, 直接淘汰

## 进化结果 (30 代 × 40 个体, 275 秒)

进化发现的最优参数 vs 手调:
| 参数 | 手调 | 进化 | 洞察 |
|------|------|------|------|
| bg_to_m1_gain | 12 | **21** | BG 需要更强驱动 M1 |
| replay_passes | 5 | **10** | 更多 SWR 重放巩固 |
| ne_floor | 0.7 | **1.0** | 永远探索, 永不利用 |
| ne_food_scale | 3.0 | **1.0** | 找到食物也不降低探索 |
| cortical_a_minus | -0.006 | **-0.011** | 更强 LTD 竞争 |
| attractor_ratio | 0.6 | **0.35** | 少定向噪声 |
| background_ratio | 0.1 | **0.24** | 更多背景活动 |

## 最终结果

| 指标 | Step 28 | Step 29 (Baldwin) | 变化 |
|------|---------|-------------------|------|
| 泛化优势 | +0.009 | **+0.667** | **74× 提升** |
| trained safety | 0.352 | **0.750** | **2×** |
| fresh safety | 0.343 | 0.083 | — |
| Learner advantage | +0.011 | **+0.017** | +55% |
| Test 4 improvement | +0.031 | **+0.496** | **16×** |
| 进化时间 | 42秒 | **275秒** | 5 seed 更慢但更准 |

## 进化最大洞察

**"永远探索, 永不利用" (ne_floor=1.0)** — 120 个神经元的小脑没有足够容量
形成稳定的利用策略, 持续探索反而更好。这类似线虫的行为: 简单神经系统靠
反射式探索而非策略性利用。

## 回归测试: 6/6 通过, 2.7 秒

## 系统状态

```
53区域 · ~120闭环神经元 · ~120投射
信息量压缩: 1100→120 (9×), 每个神经元有信息论意义
先天拓扑: Baldwin 进化 30代×40个体×5seed, 275秒完成
学习链路: ①感觉②预测编码③BG④DA⑤STDP⑥恐惧⑦海马 = 7/10 完成
泛化: +0.667 (trained=0.75 vs fresh=0.08 = 9×差距)
速度: 2.7秒/6测试, CTest 2.9秒/29套件
下一步: ⑨小脑前向预测 + ⑩丘脑主动门控
```
